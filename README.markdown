# PDF Upload and RAG Search Application

This project is a web application that allows users to upload PDF files, store their contents in a Pinecone vector database, and perform Retrieval-Augmented Generation (RAG) searches using OpenAI embeddings and GPT models. The application consists of a **Streamlit frontend** for user interaction and a **FastAPI backend** for processing uploads and queries.

## Table of Contents
- [Features](#features)
- [Architecture](#architecture)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Deployment](#deployment)
- [Project Structure](#project-structure)
- [Environment Variables](#environment-variables)
- [Contributing](#contributing)
- [License](#license)

## Features
- **PDF Upload**: Upload PDF files, process them into chunks, and store embeddings in Pinecone.
- **RAG Search**: Query the stored PDF content using natural language, with answers generated by OpenAI's GPT-4o-mini model.
- **User-Friendly Interface**: Streamlit-based frontend with tabs for uploading files and searching.
- **Secure API Key Input**: Users provide OpenAI API keys via the Streamlit sidebar, while Pinecone API keys are managed via environment variables.
- **Scalable Backend**: FastAPI handles file uploads and RAG queries efficiently.

## Architecture
- **Frontend**: A Streamlit app (`app.py`) provides a web interface for uploading PDFs and performing searches. It communicates with the FastAPI backend via HTTP requests.
- **Backend**: A FastAPI application with two routers:
  - `upload_router.py`: Handles PDF uploads, processes them using custom utilities, and stores embeddings in Pinecone.
  - `retrieve.py`: Performs RAG searches by embedding queries with OpenAI, retrieving relevant chunks from Pinecone, and generating answers with GPT-4o-mini.
- **External Services**:
  - **OpenAI**: For text embeddings (`text-embedding-3-small`) and chat completions (`gpt-4o-mini`).
  - **Pinecone**: For vector storage and similarity search.

## Prerequisites
- **Python**: Version 3.8 or higher.
- **Pinecone Account**: Obtain a Pinecone API key from [Pinecone](https://www.pinecone.io/).
- **OpenAI Account**: Obtain an OpenAI API key from [OpenAI](https://platform.openai.com/).
- **Git**: For cloning the repository.
- **Virtual Environment**: Recommended for dependency management.

## Installation
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/your-username/your-repo.git
   cd your-repo
   ```

2. **Create a Virtual Environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Dependencies**:
   Create a `requirements.txt` file with the following content:
   ```text
   fastapi
   pydantic
   uvicorn
   streamlit
   requests
   openai
   pinecone
   langchain
   langchain_openai
   langchain_pinecone
   python-dotenv
   ```
   Then install:
   ```bash
   pip install -r requirements.txt
   ```

4. **Set Up Environment Variables**:
   Create a `.env` file in the project root with the following:
   ```text
   PINECONE_API_KEY=your-pinecone-api-key
   ```

## Usage
1. **Run the FastAPI Backend**:
   Create a `main.py` file to include both routers:
   ```python
   from fastapi import FastAPI
   from retrieve import router as retrieve_router
   from upload_router import router as upload_router

   app = FastAPI()
   app.include_router(retrieve_router, prefix="/retrieve")
   app.include_router(upload_router, prefix="/files")
   ```
   Start the FastAPI server:
   ```bash
   uvicorn main:app --host 127.0.0.1 --port 4545
   ```

2. **Run the Streamlit Frontend**:
   ```bash
   streamlit run app.py
   ```
   Open your browser to `http://localhost:8501`.

3. **Using the Application**:
   - **Upload PDF**:
     - Go to the "Upload PDF" tab.
     - Enter your OpenAI API key in the sidebar.
     - Select a PDF file, specify an index name and namespace, and click "Upload".
   - **Search**:
     - Go to the "Search" tab.
     - Enter your OpenAI API key in the sidebar (if not already set).
     - Input a query, specify the index name and namespace, and click "Search" to get RAG-based answers.

## Deployment
### Streamlit Frontend (Streamlit Community Cloud)
1. **Push to GitHub**:
   - Ensure your repository includes `app.py`, `requirements.txt`, and other necessary files.
   - Push to a GitHub repository:
     ```bash
     git add .
     git commit -m "Initial commit"
     git push origin main
     ```

2. **Deploy on Streamlit Community Cloud**:
   - Log in to [Streamlit Community Cloud](https://share.streamlit.io/).
   - Create a new app, linking it to your GitHub repository.
   - Specify `app.py` as the main file.
   - In the app settings, add the environment variable:
     ```text
     API_BASE_URL=https://your-fastapi-backend-url
     ```
   - Deploy the app. Streamlit will install dependencies from `requirements.txt` without version detection issues since no versions are specified.

3. **Version Detection Note**:
   - Streamlit Community Cloud uses the `requirements.txt` file to install dependencies. Without version numbers, it installs the latest compatible versions at deployment time. To avoid issues, test locally first to ensure compatibility.

### FastAPI Backend (Render)
1. **Prepare the Backend**:
   - Ensure `main.py`, `retrieve.py`, `upload_router.py`, `requirements.txt`, and custom modules (`utils.upload_utils`, `services.pinecone_services`) are in the repository.

2. **Deploy on Render**:
   - Create a new Web Service on [Render](https://render.com/).
   - Connect your GitHub repository.
   - Set the runtime to Python and add the following environment variable:
     ```text
     PINECONE_API_KEY=your-pinecone-api-key
     ```
   - Set the start command:
     ```bash
     uvicorn main:app --host 0.0.0.0 --port $PORT
     ```
   - Deploy the service and note the public URL (e.g., `https://your-app.onrender.com`).

3. **Update Streamlit Configuration**:
   - Update the `API_BASE_URL` in your Streamlit app’s environment (or hardcode in `app.py` for testing) to the FastAPI backend’s public URL:
     ```python
     API_BASE_URL = "https://your-app.onrender.com"
     ```

### Local Testing Before Deployment
- Test the FastAPI backend:
  ```bash
  uvicorn main:app --host 127.0.0.1 --port 4545
  ```
- Test the Streamlit app:
  ```bash
  streamlit run app.py
  ```
- Ensure the API calls work by uploading a PDF and performing a search with a valid OpenAI API key.

## Project Structure
```
your-repo/
├── app.py                    # Streamlit frontend
├── main.py                   # FastAPI entry point
├── retrieve.py               # FastAPI router for RAG search
├── upload_router.py          # FastAPI router for PDF uploads
├── utils/
│   └── upload_utils.py       # Custom utilities for PDF processing
├── services/
│   └── pinecone_services.py  # Custom Pinecone index creation
├── requirements.txt          # Dependencies
├── .env                      # Environment variables (not committed)
└── README.md                 # Project documentation
```

## Environment Variables
- **PINECONE_API_KEY**: Your Pinecone API key, set in `.env` or the deployment platform.
- **API_BASE_URL**: The FastAPI backend URL (e.g., `http://127.0.0.1:4545` locally or the deployed URL). Set in Streamlit Community Cloud or modify `app.py`.

**Note**: Do not commit `.env` to version control. Add it to `.gitignore`:
```text
.env
```

## Contributing
1. Fork the repository.
2. Create a feature branch (`git checkout -b feature/your-feature`).
3. Commit changes (`git commit -m "Add your feature"`).
4. Push to the branch (`git push origin feature/your-feature`).
5. Open a pull request.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.